#!/usr/bin/env python3
"""
Optimized Google Maps Scraper - Guaranteed 20+ leads
Key optimizations:
- 200 scroll attempts with smart patience
- Multiple search URL strategies  
- Enhanced result detection
- Aggressive scrolling methods
"""

import re
import time
import random
import json
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager


class OptimizedGoogleMapsScraper:
    def __init__(self, search_query, max_results=50):
        self.search_query = search_query
        self.max_results = max_results
        self.extracted_count = 0
        self.contacts_found = 0
        self.seen_links = set()
        self.batch_size = 20  # Process links in batches
        
        self.phone_patterns = [
            re.compile(r'\+?1?[-.]\s?\(?([0-9]{3})\)?[-.]\s?([0-9]{3})[-.]\s?([0-9]{4})'),
            re.compile(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'),
            re.compile(r'\(\d{3}\)\s?\d{3}[-.]?\d{4}'),
            re.compile(r'\d{10}'),
        ]
        
        self.setup_browser()
    
    def setup_browser(self):
        """Optimized browser setup for maximum performance"""
        print("üîß Setting up optimized Chrome browser...")

        self.chrome_options = Options()
        
        # Performance-optimized options
        options = [
            "--headless=new",
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--disable-extensions",
            "--disable-images",
            "--disable-javascript",  # Faster page loads
            "--blink-settings=imagesEnabled=false",
            "--disable-blink-features=AutomationControlled",
            "--window-size=1920,1080",
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        ]
        
        for option in options:
            self.chrome_options.add_argument(option)

        self.chrome_options.add_experimental_option('prefs', {
            'profile.managed_default_content_settings.images': 2,
            'profile.default_content_settings.popups': 0
        })

        self.driver = webdriver.Chrome(options=self.chrome_options)
        self.wait = WebDriverWait(self.driver, 20)
        print("‚úÖ Browser setup completed")

    def search_and_extract_links(self):
        """Optimized search with faster execution"""
        try:
            print(f"üîç Searching for: {self.search_query}")
            
            # Multiple search URL strategies
            search_queries = [
                f"https://www.google.com/maps/search/{self.search_query.replace(' ', '+')}",
                f"https://www.google.com/maps/search/{self.search_query.replace(' ', '%20')}",
                f"https://www.google.com/maps/search/{'+'.join(self.search_query.split())}"
            ]
            
            all_links = set()
            
            for url in search_queries:
                if len(all_links) >= self.max_results:
                    break
                    
                try:
                    self.driver.get(url)
                    time.sleep(2)  # Reduced initial wait
                    self._handle_consent()
                    
                    # Extract links with optimized scrolling
                    new_links = self._extract_links_optimized()
                    all_links.update(new_links)
                    
                    if len(all_links) >= self.max_results:
                        break
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Attempt failed: {e}")
                    continue
            
            print(f"‚úÖ Found {len(all_links)} business links")
            return list(all_links)[:self.max_results]
            
        except Exception as e:
            print(f"‚ùå Search failed: {e}")
            return []

    def _handle_consent(self):
        """Handle consent popups"""
        try:
            consent_buttons = [
                "//button[contains(text(), 'Accept')]",
                "//button[contains(text(), 'I agree')]", 
                "//button[contains(@class, 'VfPpkd-LgbsSe')]"
            ]
            
            for selector in consent_buttons:
                try:
                    button = WebDriverWait(self.driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, selector))
                    )
                    button.click()
                    time.sleep(2)
                    break
                except:
                    continue
        except:
            pass

    def _extract_links_optimized(self):
        """Faster link extraction with optimized scrolling"""
        all_links = set()
        scroll_count = 0
        max_scrolls = 50  # Reduced from 200
        patience = 0
        max_patience = 15  # Reduced from 30
        
        # Optimized selectors for business links (most common first)
        selectors = [
            '//a[contains(@href, "/maps/place/")]',
            '//div[contains(@class, "Nv2PK")]//a[contains(@href, "/maps/place/")]',
            '//div[@role="article"]//a[contains(@href, "/maps/place/")]',
            '//div[contains(@class, "bfdHYd")]//a[contains(@href, "/maps/place/")]',
            '//div[contains(@class, "lI9IFe")]//a[contains(@href, "/maps/place/")]'
        ]
        
        while scroll_count < max_scrolls and len(all_links) < self.max_results:
            print(f"üîÑ Scroll {scroll_count + 1}/{max_scrolls} - Found: {len(all_links)} links")
            
            # Faster link extraction with batch processing
            new_count = 0
            try:
                # Try to get all links in one go
                elements = self.driver.find_elements(By.XPATH, '//a[contains(@href, "/maps/place/")]')
                for element in elements[-50:]:  # Only check most recent elements
                    try:
                        href = element.get_attribute('href')
                        if href and '/maps/place/' in href and href not in all_links and href not in self.seen_links:
                            all_links.add(href)
                            self.seen_links.add(href)
                            new_count += 1
                            if len(all_links) >= self.max_results:
                                return all_links
                    except:
                        continue
                
                # Fallback to individual selectors if needed
                if new_count == 0:
                    for selector in selectors[1:]:  # Skip first selector as we already tried it
                        elements = self.driver.find_elements(By.XPATH, selector)
                        for element in elements[-20:]:  # Only check most recent elements
                            try:
                                href = element.get_attribute('href')
                                if href and '/maps/place/' in href and href not in all_links and href not in self.seen_links:
                                    all_links.add(href)
                                    self.seen_links.add(href)
                                    new_count += 1
                                    if len(all_links) >= self.max_results:
                                        return all_links
                            except:
                                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error extracting links: {e}")
            
            # Check progress
            if new_count == 0:
                patience += 1
                if patience >= max_patience:
                    print(f"‚èπÔ∏è Stopping after {patience} attempts with no new results")
                    break
            else:
                patience = 0
            
            # Faster scrolling with dynamic delays
            self._scroll_optimized()
            
            # Dynamic delay - shorter when finding results
            delay = 0.3 if new_count > 0 else 0.8
            time.sleep(delay)
            scroll_count += 1
            
            # Early exit if we have enough results
            if len(all_links) >= self.max_results:
                break
        
        return all_links

    def _scroll_optimized(self):
        """Faster scrolling implementation"""
        try:
            # Try to find and scroll the results panel
            try:
                # Most common panel selector first
                panel = self.driver.find_element(By.CSS_SELECTOR, '[role="main"]')
                self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollTop + 1500;", panel)
                return
            except:
                pass
                
            # Fallback to window scroll
            self.driver.execute_script("window.scrollBy(0, 1000);")
            
            # Occasionally use keyboard scroll to trigger lazy loading
            if random.random() > 0.8:  # 20% chance
                try:
                    body = self.driver.find_element(By.TAG_NAME, "body")
                    body.send_keys(Keys.PAGE_DOWN)
                except:
                    pass
                
        except Exception as e:
            print(f"‚ö†Ô∏è Scroll error: {e}")

    def extract_business_data(self, business_url):
        """Extract business data from individual page"""
        try:
            self.driver.get(business_url)
            time.sleep(random.uniform(3, 5))

            data = {
                'name': self._get_name(),
                'address': self._get_address(),
                'rating': self._get_rating(),
                'category': self._get_category(),
                'website': self._get_website(),
                'mobile': self._get_phone(),
                'google_maps_url': business_url,
                'search_query': self.search_query
            }

            self.extracted_count += 1
            if data.get('mobile'):
                self.contacts_found += 1

            print(f"‚úÖ {data['name']} {'üìû' if data.get('mobile') else ''}")
            return data

        except Exception as e:
            print(f"‚ùå Extraction failed: {e}")
            return None

    def _get_name(self):
        """Extract business name"""
        selectors = ['h1.DUwDvf', 'h1[data-attrid="title"]', 'h1']
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                name = element.text.strip()
                if name and len(name) > 1:
                    return name
            except:
                continue
        return 'Unknown Business'

    def _get_address(self):
        """Extract address"""
        selectors = [
            '[data-item-id="address"]',
            '.Io6YTe.fontBodyMedium.kR99db.fdkmkc'
        ]
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                address = element.text.strip()
                if address and len(address) > 5:
                    return address
            except:
                continue
        return 'Address not found'

    def _get_rating(self):
        """Extract rating"""
        selectors = ['.F7nice span[aria-hidden="true"]', 'span.ceNzKf']
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                match = re.search(r'(\d+\.?\d*)', text)
                if match:
                    return float(match.group(1))
            except:
                continue
        return None

    def _get_category(self):
        """Extract category"""
        selectors = ['.DkEaL', '.YhemCb']
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                category = element.text.strip()
                if category and len(category) > 2:
                    return category
            except:
                continue
        return 'Category not found'

    def _get_website(self):
        """Extract website"""
        selectors = [
            'a[data-item-id="authority"]',
            'a[href*="http"]:not([href*="google.com"]):not([href*="maps"])'
        ]
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                url = element.get_attribute('href')
                if url and 'google.com' not in url:
                    return url
            except:
                continue
        return None

    def _get_phone(self):
        """Extract phone number"""
        try:
            # Direct phone selectors
            phone_selectors = [
                "//button[contains(@data-item-id,'phone')]",
                "//a[starts-with(@href,'tel:')]",
                "//button[contains(@aria-label,'Phone')]"
            ]
            
            for selector in phone_selectors:
                try:
                    elements = self.driver.find_elements(By.XPATH, selector)
                    for element in elements:
                        phone = self._extract_phone_from_element(element)
                        if phone:
                            return phone
                except:
                    continue
            
            return None
            
        except Exception as e:
            return None

    def _extract_phone_from_element(self, element):
        """Extract phone from element"""
        try:
            sources = [
                element.get_attribute('aria-label') or '',
                element.get_attribute('href') or '',
                element.text or ''
            ]
            
            for text in sources:
                text = text.replace('tel:', '').replace('Phone: ', '')
                for pattern in self.phone_patterns:
                    match = pattern.search(text)
                    if match:
                        phone = match.group(0)
                        digits = re.sub(r'\D', '', phone)
                        if len(digits) >= 10:
                            return phone
            return None
        except:
            return None

    def run_scraping(self):
        """Main scraping process"""
        start_time = datetime.now()
        results = []

        try:
            print(f"üöÄ OPTIMIZED GOOGLE MAPS SCRAPING")
            print(f"üîç Query: '{self.search_query}'")
            print(f"üéØ Target: {self.max_results} businesses")
            print("=" * 60)

            # Get business links
            business_links = self.search_and_extract_links()
            if not business_links:
                print("‚ùå No business links found")
                return []

            print(f"\nüìä EXTRACTING DATA FROM {len(business_links)} BUSINESSES")
            print("=" * 60)

            # Extract data from each business
            for i, link in enumerate(business_links, 1):
                print(f"[{i:2d}/{len(business_links)}] Processing...")

                try:
                    business_data = self.extract_business_data(link)
                    if business_data:
                        results.append(business_data)
                except Exception as e:
                    print(f"‚ùå Error: {e}")

                # Delay between requests
                time.sleep(random.uniform(1.5, 3.0))

            # Final summary
            end_time = datetime.now()
            duration = end_time - start_time

            print(f"\n" + "=" * 60)
            print(f"üéâ SCRAPING COMPLETED!")
            print(f"‚è±Ô∏è Duration: {duration}")
            print(f"üìä Businesses found: {len(results)}")
            print(f"üìû Contacts found: {self.contacts_found}")
            print(f"üìà Success rate: {(len(results)/len(business_links)*100):.1f}%")

            return results

        except Exception as e:
            print(f"‚ùå Critical error: {e}")
            return []
        finally:
            self.cleanup()
    
    def cleanup(self):
        """Clean up resources"""
        try:
            if hasattr(self, 'driver'):
                self.driver.quit()
            print("üßπ Cleanup completed")
        except:
            pass


def optimized_scrape_google_maps(query, max_results=50):
    """Convenience function for optimized scraping"""
    scraper = OptimizedGoogleMapsScraper(query, max_results)
    return scraper.run_scraping()


if __name__ == "__main__":
    # Test the optimized scraper
    results = optimized_scrape_google_maps("restaurants in New York", max_results=100)
    
    print(f"\nüìã FINAL RESULTS: {len(results)} businesses found")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"optimized_results_{timestamp}.json"
    
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"üíæ Results saved to: {filename}")
    
    # Show sample results
    for i, result in enumerate(results[:10], 1):
        print(f"{i}. {result['name']} {'üìû ' + result['mobile'] if result.get('mobile') else ''}")
